{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a705d20",
   "metadata": {},
   "source": [
    "# Lab1 ‚Äî PyTorch Foundations for Computer Vision\n",
    "\n",
    "**Course**: Deep Learning for Image Analysis \n",
    "\n",
    "**Class**: M2 IASD App  \n",
    "\n",
    "**Professor**: Mehyar MLAWEH\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "- Understand how **neurons and layers** are implemented in PyTorch\n",
    "- Manipulate **tensors** and reason about shapes\n",
    "- Use **autograd** to compute gradients\n",
    "- Implement a **training loop** yourself\n",
    "- Connect theory (neurons, loss, backprop) to actual code\n",
    "\n",
    "‚ö†Ô∏è This notebook is **intentionally incomplete**.  \n",
    "Whenever you see **`# TODO`**, you are expected to write code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07470cd",
   "metadata": {},
   "source": [
    "\n",
    "**Deadline:** üóìÔ∏è **Saturday, February 7th (23:59)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60119f3a",
   "metadata": {},
   "source": [
    "## ü§ñ A small (honest) note before you start\n",
    "\n",
    "Let‚Äôs be real for a second.\n",
    "\n",
    " I know you **can use LLMs (ChatGPT, Copilot, Claude, etc.)** to help you with this lab.  \n",
    "And yes, **I use them too**, so don‚Äôt worry üòÑ\n",
    "\n",
    "üëâ **You are allowed to use AI tools.**  \n",
    "But here‚Äôs the deal:\n",
    "\n",
    "- Don‚Äôt just **copy‚Äìpaste** code you don‚Äôt understand  \n",
    "- Take time to **read, question, and modify** what the model gives you  \n",
    "- If you can solve a block **by yourself, without AI**, that‚Äôs excellent \n",
    "\n",
    "Remember:\n",
    "\n",
    "> AI can write code for you, but **only you can understand it** ‚Äî and understanding is what matters for exams, projects, and real work.\n",
    "\n",
    "Use these tools **as assistants, not as replacements for thinking**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Useful documentation (highly recommended)\n",
    "\n",
    "You will often find answers faster (and more reliably) by checking the official documentation:\n",
    "\n",
    "- **PyTorch main documentation**  \n",
    "  https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "- **PyTorch tensors**  \n",
    "  https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "- **Neural network modules (`torch.nn`)**  \n",
    "  https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "- **Loss functions** (`BCEWithLogitsLoss`, CrossEntropy, etc.)  \n",
    "  https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "- **Optimizers** (`SGD`, `Adam`, ‚Ä¶)  \n",
    "  https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "If you learn how to **navigate the documentation**, you are already thinking like a real AI engineer üëå\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278eff5",
   "metadata": {},
   "source": [
    "## PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de614322",
   "metadata": {},
   "source": [
    "## 0) Colab setup ‚Äî GPU check\n",
    "\n",
    "**Instructions**\n",
    "1. In Colab: `Runtime ‚Üí Change runtime type to GPU T4` \n",
    "2. Select **GPU**\n",
    "3. Save and restart runtime\n",
    "\n",
    "Then run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# TODO: set the device correctly (cuda if available, else cpu)\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7ceb1",
   "metadata": {},
   "source": [
    "## 1) Imports and reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ce2798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2376f4da6f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# TODO: fix the random seed for reproducibility\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 3 cons√©quences principales :\n",
    "# - initialisation des poids : √† chaque fois que l'on va r√©entrainer le m√™me r√©seau de neurones, les poids des neurones seront initialis√©s de la m√™me mani√®re\n",
    "# - m√©lange des donn√©es : √† chaque √©poque de l'entra√Ænement, les donn√©es seront m√©lang√©s de la m√™me mani√®re\n",
    "# - dropout : en cas de dropout, les neurones d√©sactiv√©s seront toujours les m√™mes √† chaque essai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349f5a5",
   "metadata": {},
   "source": [
    "## 2) PyTorch tensors and shapes\n",
    "\n",
    "Tensors are multi-dimensional arrays that support:\n",
    "- GPU acceleration\n",
    "- automatic differentiation\n",
    "\n",
    "Understanding **shapes** is critical in deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2998b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: torch.Size([3])\n",
      "b shape: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "a = torch.tensor([1.0, 2.0, 3.0]) # vecteur de taille 3\n",
    "b = torch.randn(4, 5) # matrice 4 x 5 remplie al√©atoirement selon N(0,1)\n",
    "\n",
    "print(\"a shape:\", a.shape)\n",
    "print(\"b shape:\", b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d675977",
   "metadata": {},
   "source": [
    "### üîç Question (answer inside the markdown)\n",
    "- How many dimensions does tensor `b` have? It has two dimensions\n",
    "- What does each dimension represent conceptually? La question est ambigue...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0588f",
   "metadata": {},
   "source": [
    "### ‚úÖTensor operations\n",
    "\n",
    "Complete the following:\n",
    "\n",
    "1. Create a tensor `x` of shape `(8, 3)` with random values  \n",
    "2. Compute:\n",
    "   - the **mean of each column**\n",
    "   - the **L2 norm of each row**\n",
    "3. Normalize `x` **row-wise** using the L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4629e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4138,  0.5184, -0.7015],\n",
      "        [-0.4323,  0.1415,  0.0711],\n",
      "        [ 0.5634, -0.5786, -0.9437],\n",
      "        [ 0.1730, -1.8815,  0.5851],\n",
      "        [ 1.5287, -0.9324,  1.3527],\n",
      "        [ 0.1603,  0.5374,  0.7817],\n",
      "        [ 1.0477, -0.3948,  1.6077],\n",
      "        [-0.8064,  0.0732, -2.0952]])\n",
      "tensor([[-0.4286,  0.5370, -0.7266],\n",
      "        [-0.9390,  0.3073,  0.1544],\n",
      "        [ 0.4535, -0.4659, -0.7598],\n",
      "        [ 0.0875, -0.9512,  0.2958],\n",
      "        [ 0.6812, -0.4155,  0.6028],\n",
      "        [ 0.1666,  0.5586,  0.8126],\n",
      "        [ 0.5348, -0.2015,  0.8206],\n",
      "        [-0.3590,  0.0326, -0.9328]])\n",
      "torch.Size([8, 3]) torch.Size([3]) torch.Size([8, 1]) torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "# TODO: create x\n",
    "x = torch.randn(8,3)\n",
    "print(x)\n",
    "\n",
    "# TODO: column mean\n",
    "col_mean = torch.mean(x, 0) # torch.mean(input, dim, keepdim=False, *, dtype=None, out=None) \n",
    "# -> 'dim' correspond √† la dimension que l'on va \"√©craser\" en voulant calculer la moyenne. Pour calculer la moyenne des colonnes, on \"√©crase\" les lignes, d'o√π dim=0.\n",
    "# -> ATTENTION : dim=d va litt√©ralement √©craser la dimension d, i.e. la supprimer du tenseur d'output. Utiliser keepdim=True pour conserver la dimension.\n",
    "\n",
    "# TODO: row-wise L2 norm\n",
    "row_norm = torch.linalg.norm(x, dim=1, keepdim=True) # keepdim=True indispensable pour la normalisation suivante\n",
    "\n",
    "# TODO: normalized tensor\n",
    "x_normalized = x / row_norm\n",
    "print(x_normalized)\n",
    "\n",
    "print(x.shape, col_mean.shape, row_norm.shape, x_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a9540",
   "metadata": {},
   "source": [
    "Visualisation de la normalisation\n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_{81} & x_{82} & x_{83}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$row\\_norm = \\begin{pmatrix}\n",
    "n_1 \\\\\n",
    "n_2 \\\\\n",
    "\\vdots \\\\\n",
    "n_8\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$X_{norm} = \\begin{pmatrix}\n",
    "\\frac{x_{11}}{n_1} & \\frac{x_{12}}{n_1} & \\frac{x_{13}}{n_1} \\\\\n",
    "\\frac{x_{21}}{n_2} & \\frac{x_{22}}{n_2} & \\frac{x_{23}}{n_2} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\frac{x_{81}}{n_8} & \\frac{x_{82}}{n_8} & \\frac{x_{83}}{n_8}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f8928",
   "metadata": {},
   "source": [
    "## 3) Artificial neuron ‚Äî from math to code\n",
    "\n",
    "A neuron computes:\n",
    "\n",
    "$$\n",
    "z = \\sum_i w_i x_i + b\n",
    "$$\n",
    "\n",
    "Then applies an activation function:\n",
    "\n",
    "$$\n",
    "y = g(z)\n",
    "$$\n",
    "\n",
    "This section connects directly to the theory seen in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d271c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, -2.0, 3.0])\n",
    "w = torch.tensor([0.2, 0.4, -0.1])\n",
    "b = torch.tensor(0.1)\n",
    "\n",
    "z = torch.sum(x * w) + b\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d7490",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "1. Implement **ReLU**\n",
    "2. Implement **Sigmoid**\n",
    "3. Apply both to `z` and compare the outputs\n",
    "\n",
    "Which activation preserves negative values?\n",
    "-> AUCUNE des deux. Pour pr√©server les valeurs n√©gatives il y a Leaky ReLU ou Tanh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f307df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(0.3318, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "def relu(z):\n",
    "    return max(0,z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+torch.exp(-z))\n",
    "\n",
    "y_relu = relu(z)\n",
    "y_sigmoid = sigmoid(z)\n",
    "y_relu, y_sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e764019",
   "metadata": {},
   "source": [
    "## 4) Autograd and gradients\n",
    "\n",
    "PyTorch uses **automatic differentiation** to compute gradients\n",
    "using the **chain rule** (backpropagation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.890000104904175\n",
      "grad w: tensor([-3.4000, -6.8000,  3.4000])\n",
      "grad b: tensor(-3.4000)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, -1.0], requires_grad=True) # requires_grad=True permet de dire √† PyTorch de retenir toutes les op√©rations effectu√©es sur le tenseur\n",
    "w = torch.tensor([0.5, -0.3, 0.8], requires_grad=True)\n",
    "b = torch.tensor(0.2, requires_grad=True)\n",
    "\n",
    "z = torch.sum(x * w) + b\n",
    "loss = (z - 1.0) ** 2\n",
    "\n",
    "loss.backward() # .backward() d√©clenche la phase de r√©tropropagation\n",
    "# -> avant .backward(), w.grad = None, b.grad = None\n",
    "# -> apr√®s .backward(), w.grad et b.grad contiennent les valeurs du gradient calcul√©\n",
    "\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"grad w:\", w.grad)\n",
    "print(\"grad b:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c78a9",
   "metadata": {},
   "source": [
    "### üîç Conceptual question\n",
    "\n",
    "- If `b.grad > 0`, should `b` increase or decrease after a gradient descent step?\n",
    "Explain **why** in one sentence.<br><br>\n",
    "\n",
    "$b.grad = \\frac{\\partial L}{\\partial b}$ donc si b.grad est positif, la loss augmente. Il faut donc diminuer b √† l'√©tape suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdee3e",
   "metadata": {},
   "source": [
    "## 5) Toy classification dataset\n",
    "\n",
    "We create a **linearly separable** dataset.\n",
    "\n",
    "Label rule:\n",
    "- class = 1 if `x‚ÇÅ + x‚ÇÇ + x‚ÇÉ > 0`\n",
    "- class = 0 otherwise\n",
    "\n",
    "This mimics a very simple classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c8bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate a dataset of size N=500 with 3 features\n",
    "\n",
    "N=500\n",
    "X = torch.randn(N,3)\n",
    "y = torch.where(torch.sum(X, dim=1) > 0, 1, 0) # comment cr√©er la condition ? torch.where(condition, input, other, *, out=None) -> retourne 'input' si condition=True, 'other' sinon\n",
    "\n",
    "# TODO: split into train (80%) and validation (20%)\n",
    "\n",
    "split_size = int(0.8*N)\n",
    "\n",
    "X_train = X[:split_size , :]\n",
    "X_val = X[split_size: , :]\n",
    "\n",
    "y_train = y[:split_size]\n",
    "y_val = y[split_size:]\n",
    "\n",
    "# ATTENTION : on suppose que les donn√©es sont m√©lang√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c16fc2",
   "metadata": {},
   "source": [
    "## 6) Model definition\n",
    "\n",
    "We define a small **MLP** (fully-connected network):\n",
    "\n",
    "`3 ‚Üí 16 ‚Üí 8 ‚Üí 1`\n",
    "\n",
    "Activation: ReLU  \n",
    "Output: raw logits (no sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7b69f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# TODO: create model and move it to the GPU\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13b2d2",
   "metadata": {},
   "source": [
    "###  parameters\n",
    "\n",
    "1. Compute **by hand** the total number of parameters\n",
    "\n",
    "Le nombre de param√®tres du r√©seau de neurones devrait √™tre : (3 x 16 + 16) + (16 x 8 + 8) + (8 x 1 + 1) = 209\n",
    "\n",
    "2. Verify your answer using PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6168e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de param√®tres : 209\n"
     ]
    }
   ],
   "source": [
    "# TODO: count parameters with PyTorch\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Nombre total de param√®tres : {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f204fb",
   "metadata": {},
   "source": [
    "## 7) Training loop \n",
    "\n",
    "You must complete the full training loop:\n",
    "- forward pass\n",
    "- loss computation\n",
    "- backward pass\n",
    "- optimizer step\n",
    "\n",
    "Loss: `BCEWithLogitsLoss`<br>\n",
    "Optimizer: `SGD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ad2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | loss = 0.6947555541992188\n",
      "Epoch 5 | loss = 0.689077615737915\n",
      "Epoch 10 | loss = 0.6831324100494385\n",
      "Epoch 15 | loss = 0.676664412021637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivo-\\AppData\\Local\\Temp\\ipykernel_13248\\2731340803.py:27: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  print(\"Epoch\", epoch, \"| loss =\", float(loss))\n"
     ]
    }
   ],
   "source": [
    "# TODO: move data to device\n",
    "X_train_d = X_train.to(device)\n",
    "y_train_d = y_train.to(device).float().unsqueeze(1) # .float() n√©cessaire car le torch.where() a renvoy√© des Long (entiers), or PyTorch veut des floats pour que la fonction de perte puisse calculer les gradients\n",
    "X_val_d = X_val.to(device)\n",
    "y_val_d = y_val.to(device).float().unsqueeze(1)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # OBLIGATOIRE pour ne pas cumuler les gradients des √©poques pr√©c√©dentes\n",
    "\n",
    "    # TODO: forward\n",
    "    logits = model.forward(X_train_d)\n",
    "\n",
    "    # TODO: loss\n",
    "    loss = criterion(logits, y_train_d)\n",
    "\n",
    "    # TODO: backward\n",
    "    loss.backward()\n",
    "\n",
    "    # TODO: update\n",
    "    optimizer.step() # c'est ICI qu'on applique la formule de la descente de gradient pour mettre √† jour les poids\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch\", epoch, \"| loss =\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d771",
   "metadata": {},
   "source": [
    "Remarque :<br>\n",
    "\n",
    "`model.train()` : cette instruction dit √† PyTorch comment certaines couches doivent se comporter. En effet, certaines couches PyTorch ne se comportent pas de le m√™me mani√®re en entra√Ænement qu'en √©valuation (ex : nn.Dropout(), nn.BatchNorm1d()). Par exemple, en mode train, la couche de Dropout d√©sactive des neurones alors qu'en mode eval tous les neurones sont actifs.<br>\n",
    "\n",
    "Dans ce code, l'instruction `model.train()` n'est pas obligatoire mais c'est une bonne pratique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c894744",
   "metadata": {},
   "source": [
    "## 8) Evaluation\n",
    "\n",
    "1. Apply `sigmoid` to the logits\n",
    "2. Convert probabilities to predictions\n",
    "3. Compute **accuracy** on the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7400)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: evaluation\n",
    "with torch.no_grad(): # .no_grad() d√©sactive le suivi des op√©rations sur les tenseurs o√π l'on avait √©crit \"requires_grad=True\" (cela r√©duit l'utilisation de la m√©moire)\n",
    "    logits = model.forward(X_val_d)\n",
    "    probs = sigmoid(logits)\n",
    "    preds = torch.where(probs > 0.5, 1, 0)\n",
    "\n",
    "accuracy = (preds == y_val_d).float().mean()\n",
    "# -> preds == y_val_d renvoie un tenseur de bool√©ens\n",
    "# -> .float() convertit ces bool√©ens en float : True  = 1.0, False = 0.0\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698541c",
   "metadata": {},
   "source": [
    "## 9) Reflection questions (answer inside the markdown)\n",
    "\n",
    "1. Why do we **not** apply sigmoid inside the model?\n",
    "\n",
    "Stabilit√© : BCEWithLogitsLoss calcule les gradients de fa√ßon plus pr√©cise qu'une Sigmoid manuelle.\n",
    "\n",
    "Saturation : On √©vite que les gradients deviennent minuscules trop vite pendant l'entra√Ænement.\n",
    "\n",
    "2. What would happen if we removed all ReLU activations?\n",
    "\n",
    "Il ne pourra plus apprendre que des droites et √©chouera sur des probl√®mes complexes.\n",
    "\n",
    "3. How does this toy problem relate to image classification?\n",
    "\n",
    "C'est qu'une question d'√©chelle. Au lieu d'avoir des tenseurs de dimension 2, on aura des tenseurs de dimension 3 (la troisi√®me dimension correspondant aux nombre de couleurs, ou canaux)\n",
    "\n",
    "Write short answers (2‚Äì3 lines each).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f2ed3",
   "metadata": {},
   "source": [
    "## 10) Bridge to Computer Vision\n",
    "\n",
    "So far:\n",
    "- inputs = vectors of size 3\n",
    "- layers = fully-connected\n",
    "\n",
    "Next session:\n",
    "- inputs = images `(B, C, H, W)`\n",
    "- layers = convolutions\n",
    "- same training logic\n",
    "\n",
    "üëâ **Architecture changes, learning principles stay the same.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479aad6",
   "metadata": {},
   "source": [
    "## Part II ‚Äî Training on MNIST\n",
    "\n",
    "Check the next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
