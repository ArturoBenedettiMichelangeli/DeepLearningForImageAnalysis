{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b1a5ee9",
      "metadata": {
        "id": "4b1a5ee9"
      },
      "source": [
        "# Lab1 ‚Äî Part 2: Training on MNIST\n",
        "\n",
        "**Course**: Deep Learning for Image Analysis\n",
        "\n",
        "**Class**: M2 IASD App  \n",
        "\n",
        "**Professor**: Mehyar MLAWEH\n",
        "\n",
        "---\n",
        "\n",
        "In this part, we move from a **toy dataset** to a **real image dataset**.  \n",
        "We will train a neural network on **MNIST**, **without using CNNs**.\n",
        "\n",
        "The goal is to understand **why CNNs are needed**, not to use them yet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18d7b8d",
      "metadata": {
        "id": "f18d7b8d"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "After this part, you should understand:\n",
        "- How images are represented as **vectors**\n",
        "- How a neural network processes image data\n",
        "- How multi-class classification works\n",
        "- Why fully-connected networks are **not optimal** for vision tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa93da68",
      "metadata": {
        "id": "aa93da68"
      },
      "source": [
        "## 0) Environment setup\n",
        "\n",
        "Make sure you are using **GPU**:\n",
        "- Runtime ‚Üí Change runtime type ‚Üí GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fdb2e7a0",
      "metadata": {
        "id": "fdb2e7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9f62ce",
      "metadata": {
        "id": "5e9f62ce"
      },
      "source": [
        "## 1) Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90c57b10",
      "metadata": {
        "id": "90c57b10"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms # sert au pr√©traitement de donn√©es\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b05070c",
      "metadata": {
        "id": "7b05070c"
      },
      "source": [
        "## 2) The MNIST dataset\n",
        "\n",
        "MNIST contains **handwritten digit images**:\n",
        "- Image size: **28 √ó 28**\n",
        "- Number of channels: **1 (grayscale)**\n",
        "- Number of classes: **10** (digits 0‚Äì9)\n",
        "\n",
        "Each image is originally a **2D grid of pixels**, but a fully-connected network\n",
        "expects a **vector** as input.\n",
        "\n",
        "https://www.youtube.com/watch?v=SrT6QkQUH4Q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4148a32",
      "metadata": {
        "id": "b4148a32"
      },
      "source": [
        "### üîç Question (conceptual)\n",
        "\n",
        "- How many pixels does one MNIST image contain? -> 28*28 = 784\n",
        "- What will be the size of the input vector after flattening? -> (784,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "08926df3",
      "metadata": {
        "id": "08926df3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:02<00:00, 3.79MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 716kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 2.64MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 4.35MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "len(train_dataset), len(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36_lu-a5IMdN",
      "metadata": {
        "id": "36_lu-a5IMdN"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x300 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "num_images = 6\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "\n",
        "for i in range(num_images):\n",
        "    image, label = train_dataset[i]\n",
        "\n",
        "    plt.subplot(1, num_images, i + 1)\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Sample images from the MNIST training dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdc2aec",
      "metadata": {
        "id": "ccdc2aec"
      },
      "source": [
        "## 3) DataLoader and mini-batches\n",
        "\n",
        "We use **mini-batches** to train efficiently.\n",
        "\n",
        "‚ö†Ô∏è Choosing the batch size is a trade-off between speed and memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ef0fb336",
      "metadata": {
        "id": "ef0fb336"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a9763a",
      "metadata": {
        "id": "a6a9763a"
      },
      "source": [
        "## 4) Model: Fully-connected neural network (MLP)\n",
        "\n",
        "### Two experiments with the same model\n",
        "\n",
        "In this part, we train **the same neural network (MLP)** on two versions of MNIST:\n",
        "\n",
        "1. **Original MNIST** (centered digits)\n",
        "2. **Transformed MNIST** (shifted and rotated digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1DIP-o04J2v4",
      "metadata": {
        "id": "1DIP-o04J2v4"
      },
      "source": [
        "#### Experiment 1: MNIST (baseline)\n",
        "\n",
        "1. **Flatten** the image `(1, 28, 28)` ‚Üí `(784,)`\n",
        "2. Feed it to a standard MLP\n",
        "\n",
        "Architecture:\n",
        "- Input: 784\n",
        "- Hidden layer: 256 neurons + ReLU\n",
        "- Output: 10 neurons (one per class)\n",
        "\n",
        "The output represents **class scores (logits)**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "AJh0xy8TJAPP",
      "metadata": {
        "id": "AJh0xy8TJAPP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP_MNIST(\n",
              "  (net): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MLP_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28 * 28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP_MNIST().to(device)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d640f1",
      "metadata": {
        "id": "b0d640f1"
      },
      "source": [
        "‚úÖ TODO ‚Äî Understanding the model\n",
        "\n",
        "1. How many parameters does this model have? -> La couche .Flatten() en poss√®de aucun param√®tres, donc au total on a (784 x 256 + 256) + (256 x 10 + 10) = 203530\n",
        "2. Which layer contains most of the parameters? Explain briefly. -> La couche lin√©aire contient la majorit√© des param√®tres.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c3156cc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre total de param√®tres : 203530\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Nombre total de param√®tres : {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c8b7b9e",
      "metadata": {
        "id": "6c8b7b9e"
      },
      "source": [
        " **Loss function and optimizer**\n",
        "\n",
        "This is a **multi-class classification** problem.\n",
        "\n",
        "- We use `CrossEntropyLoss`\n",
        "- It combines **Softmax + Negative Log-Likelihood**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0085e6bb",
      "metadata": {
        "id": "0085e6bb"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9c21bd",
      "metadata": {
        "id": "bf9c21bd"
      },
      "source": [
        "\n",
        "**Training loop**\n",
        "\n",
        "The training logic is **exactly the same** as before.\n",
        "Only the data and the model changed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b1ee4336",
      "metadata": {
        "id": "b1ee4336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Loss: 0.3596\n",
            "Epoch 2/3 - Loss: 0.1585\n",
            "Epoch 3/3 - Loss: 0.1071\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa7acb6",
      "metadata": {
        "id": "ffa7acb6"
      },
      "source": [
        "**Evaluation on the test set**\n",
        "\n",
        "We compute the **classification accuracy**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23546b5e",
      "metadata": {
        "id": "23546b5e"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LWH5pHVpKyu9",
      "metadata": {
        "id": "LWH5pHVpKyu9"
      },
      "source": [
        "#### Experiment 2: Shifted and rotated MNIST\n",
        "\n",
        "We now apply random shifts and rotations to the training images.\n",
        "\n",
        "This breaks the assumption that digits are always centered and aligned.\n",
        "\n",
        "The **model architecture remains exactly the same**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aK0e_676KyVP",
      "metadata": {
        "id": "aK0e_676KyVP"
      },
      "outputs": [],
      "source": [
        "transform_augmented = transforms.Compose([\n",
        "    transforms.RandomAffine(\n",
        "        degrees=50,\n",
        "        translate=(0.2, 0.2)\n",
        "    ),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset_aug = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_augmented\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "len(train_dataset_aug), len(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kwVY61ZQLF8M",
      "metadata": {
        "id": "kwVY61ZQLF8M"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_images = 6\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "\n",
        "for i in range(num_images):\n",
        "    image, label = train_dataset_aug[i]\n",
        "\n",
        "    plt.subplot(1, num_images, i + 1)\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Sample images from the MNIST training dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LQbnHv16LeU2",
      "metadata": {
        "id": "LQbnHv16LeU2"
      },
      "source": [
        "Train the **same model** using the augmented training dataset.\n",
        "\n",
        "Compare:\n",
        "- Training loss\n",
        "- Test accuracy\n",
        "\n",
        "with the results obtained using original MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x_U5Kdc4LhR5",
      "metadata": {
        "id": "x_U5Kdc4LhR5"
      },
      "outputs": [],
      "source": [
        "# Initialisation du mod√®le, du crit√®re et de l'optimiseur\n",
        "# On repart d'un mod√®le neuf pour ne pas biaiser l'exp√©rience\n",
        "model_aug = MLP_MNIST().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_aug.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model_aug.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader_aug:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_aug(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader_aug)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss (Augmented): {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1C-JESHjMdLD",
      "metadata": {
        "id": "1C-JESHjMdLD"
      },
      "outputs": [],
      "source": [
        "model_aug.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader: # Utilise le test_loader standard\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_aug(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy_aug = correct / total\n",
        "print(f\"Accuracy on original test set after augmented training: {accuracy_aug:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e17172",
      "metadata": {
        "id": "17e17172"
      },
      "source": [
        "## 5) What do we observe? (Reflection)\n",
        "\n",
        " - Why does the same fully-connected model perform worse on shifted and rotated MNIST images compared to vanilla MNIST?\n",
        "<br>\n",
        "Le mod√®le MLP est sensible √† la position exacte des pixels. Si un \"7\" est d√©cal√© de 5 pixels, il active des entr√©es (neurones) totalement diff√©rentes de celles apprises lors de l'entra√Ænement, car le r√©seau ne comprend pas la notion de \"voisinage\" spatial.\n",
        "\n",
        " - What limitation of fully-connected networks does this experiment reveal when dealing with images?\n",
        "<br>\n",
        "L'exp√©rience r√©v√®le le manque d'invariance spatiale (ou invariance par translation). Pour un MLP, une image n'est qu'un vecteur de pixels ind√©pendants ; il ne poss√®de pas de m√©canisme pour reconna√Ætre un motif s'il n'est pas exactement √† l'endroit pr√©vu.\n",
        "- Based on these results, what properties should a better model for image data have?<br>\n",
        "Un meilleur mod√®le (comme le CNN) devrait poss√©der l'invariance par translation et la capacit√© d'extraire des caract√©ristiques locales (bords, boucles) qui sont valables peu importe leur position dans l'image.\n",
        "\n",
        "‚úçÔ∏è Write short answers (2‚Äì3 lines).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068078ff",
      "metadata": {
        "id": "068078ff"
      },
      "source": [
        "## 6) Looking ahead ‚Äî why CNNs?\n",
        "\n",
        "In the next assignment, we will replace this MLP with **Convolutional Neural Networks**.\n",
        "\n",
        "CNNs:\n",
        "- Exploit **spatial structure**\n",
        "- Share parameters\n",
        "- Achieve **much higher accuracy** on image tasks\n",
        "\n",
        "üëâ You should expect a **significant accuracy improvement** in the next assignment.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
